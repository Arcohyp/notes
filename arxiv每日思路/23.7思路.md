## 2023年7月
### MiVOLO: Multi-input Transformer for Age and Gender Estimation
extension://idghocbbahafpfhjnfhpbfbmpegphmmp/assets/pdf/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2307.04616v1.pdf

在2stage VOLO模型前面增加一个特征增强模块，使用的是cross attention，用k,v处理body，用q处理face。至于如何提取face信息，是用yolov8处理的

### Swin Transformer 模型
Swin Transformer是一种用于图像处理的基于Transformer架构的深度学习模型。它在2021年由香港中文大学和senseTime公司共同提出，并在CVPR 2021上进行了发布。

传统的Transformer模型在处理图像任务时会面临一些挑战，其中一个主要问题是高分辨率图像的处理效率问题。Swin Transformer通过引入分级的窗口化注意力机制来解决这个问题。它将图像划分为一系列大小相等的非重叠的小块，称为窗口，然后在每个窗口内应用Transformer的自注意力机制。通过这种方式，Swin Transformer能够更高效地处理大型图像。

此外，Swin Transformer还引入了跨阶段的Transformer编码器结构（Shifted Window Transformer）来捕捉不同尺度和分辨率的特征。它通过将图像的特征分解为多个阶段，并在不同阶段之间进行信息交换，以建立全局和局部之间的联系。

Swin Transformer在多个图像相关任务上取得了很好的性能，包括图像分类、目标检测和语义分割。它相比于传统的卷积神经网络，在准确性和可扩展性方面都表现出了很大的提升。通过引入窗口化注意力机制和跨阶段的Transformer编码器结构，Swin Transformer为图像处理任务带来了新的可能性，并在学术界和工业界引起了广泛的关注。

### VOLO 模型
VOLO（Vision Outlooker for Visual Recognition）是一种基于视觉的远见模型，用于图像分类和目标检测任务。它于2021年由香港中文大学和微软亚洲研究院共同提出。

VOLO模型旨在解决传统视觉模型在处理大尺度图像和远距离目标时的局限性。传统模型在这些情况下容易缺乏空间上下文的感知能力，导致性能下降。VOLO通过引入两个关键组件来解决这个问题：视觉地理位置编码和远见注意力模块。

视觉地理位置编码是一种将图像中的位置信息编码到特征表示中的方法。它为每个位置引入自适应的地理位置编码，使模型能够更好地理解和利用不同位置的空间上下文。

远见注意力模块是一种新的注意力机制，在提取特征时，能够显式地关注大尺度的上下文信息。它通过对特征图进行多尺度的分组和交互来促进信息传递，从而扩大模型的感受野，并提高性能。

VOLO模型在多个大规模图像分类和目标检测数据集上进行了实验，并取得了优秀的性能。它在不同尺度和远距离目标的处理上具有优势，有效地提高了图像理解任务的精度和泛化能力。

总之，VOLO模型通过引入视觉地理位置编码和远见注意力模块，为视觉任务带来了更好的上下文感知能力，并在图像分类和目标检测任务中取得了优秀的表现。它为解决大尺度图像处理的挑战提供了一种新的方法。

### PKU-GoodsAD: A Supermarket Goods Dataset for Unsupervised Anomaly Detection and Segmentation

https://arxiv.org/pdf/2307.04956v1.pdf

构建了一个关于超市商品的数据集，并且包含完整商品和“损伤”商品的内容。

使用三大类方式进行测试，

1）基于预训练模型

2）基于异常点

3）基于生成式模型

最终对上述所有方法进行比较，最终得出结论，PatchCore方法效果最好

### Q-YOLO: Efficient Inference for Real-time Object Detection

extension://idghocbbahafpfhjnfhpbfbmpegphmmp/assets/pdf/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2307.04816v1.pdf

### 量化

https://zhuanlan.zhihu.com/p/149659607

量化并不是什么新知识，我们在对图像做预处理时就用到了量化。回想一下，我们通常会将一张 uint8 类型、数值范围在 0\~255 的图片归一成 float32 类型、数值范围在 0.0\~1.0 的张量，这个过程就是反量化。类似地，我们经常将网络输出的范围在 0.0\~1.0 之间的张量调整成数值为 0\~255、uint8 类型的图片数据，这个过程就是量化。所以量化本质上只是对数值范围的重新调整，可以「粗略」理解为是一种线性映射。(之所以加「粗略」二字，是因为有些论文会用非线性量化，但目前在工业界落地的还都是线性量化，所以本文只讨论线性量化的方案)。

https://zhuanlan.zhihu.com/p/64744154

### PTQ 模型
PTQ模型是指“Post Training Quantization”的缩写，它是一种用于对神经网络进行量化的技术。量化是指将浮点数模型参数转化为定点数表示的过程，可以显著减少模型的内存占用和计算量，从而提高模型在移动设备和嵌入式系统上的部署效率。

在PTQ模型中，首先使用浮点数模型进行训练，得到经过训练优化的模型。然后，通过对训练后的模型进行量化，将模型中的浮点数参数转换为较低精度的定点数表示。通常情况下，常用的量化方法包括对权重进行缩放和截断，以及对激活函数进行近似等。

量化后的模型可以在硬件上更高效地执行，因为定点数能够节省内存带宽和计算资源。然而，由于量化引入了信息损失，所以在进行量化之前需要仔细考虑量化策略，以平衡模型的精度和性能。一般情况下，需要在保持模型准确性的前提下，尽量降低量化引入的误差。

总之，PTQ模型是一种通过量化神经网络，将浮点数表示转换为定点数表示的技术，以提高模型在移动设备和嵌入式系统上的部署效率。

### 自监督学习

https://zhuanlan.zhihu.com/p/184995155

自监督学习（Self-supervised learning）是一种无需人工标注标签的机器学习方法，它通过自动生成训练目标或使用数据自身的内部关联来进行学习。相比监督学习需要大量标记的数据集，自监督学习可以更有效地利用未标记的数据。

在自监督学习中，模型通过尝试解决一个自我设定的任务来学习表示。这个任务可以是对数据进行转换、预测缺失的部分、生成原始数据等等。通过让模型从数据中学习这些任务，可以让其学到有用的特征表示，同时也能提高模型在下游任务上的表现。

举个例子，对于图像领域的自监督学习，可以通过将图像进行旋转、遮挡部分区域等方式来创建自我设定的任务。模型需要学会将旋转后的图像恢复到原始方向或者预测被遮挡的部分。通过这个过程，模型能够学习到图像中的语义信息和结构，并可以在下游任务（如图像分类、目标检测等）上产生更好的性能。

自监督学习的优势在于不需要手动标注标签，可以利用大量未标记的数据，在许多领域都取得了很好的效果。它被广泛应用于计算机视觉、自然语言处理等领域，为机器学习提供了一种更加高效和灵活的学习范式。
