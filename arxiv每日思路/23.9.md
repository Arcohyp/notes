# 23.9
> https://scholar.lanfanshu.cn/citations?hl=zh-CN&user=Vg54TcsAAAAJ&view_op=list_works&sortby=pubdate

## [1] Shen, Leixian, et al. “Towards natural language interfaces for data visualization: A survey.” IEEE transactions on visualization and computer graphics (2022).

## [2] Driess, Danny, et al. “Palm-e: An embodied multimodal language model.” arXiv preprint arXiv:2303.03378 (2023).

    extension://idghocbbahafpfhjnfhpbfbmpegphmmp/assets/pdf/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2303.03378.pdf
> https://www.connectedpapers.com/main/38fe8f324d2162e63a967a9ac6648974fc4c66f3/PaLM%20E%3A-An-Embodied-Multimodal-Language-Model/graph

- 某人锐评，小demo来骗？
> https://www.bilibili.com/video/BV1As4y157vp/?spm_id_from=333.337.search-card.all.click
>
>> 然后呢？谷歌还不是把Everyday Robots给解散了。
>
>> 他们的文章里面的实验一部分都是toy problems,稍微复杂点的任务，基本就歇菜了，但是却有意无意暗示“通才”！至少视觉编码器就得针对特定任务进行训练。再加上他们也不开源（估计是不敢），所以其他团队根本没法评价比较他们的成果。
>
>> 现在的谷歌做AI就这种心态，只追求Demo漂亮水文章，怪不得bard翻车。

## [3] Lin, Kevin, et al. “Text2motion: From natural language instructions to feasible plans.” arXiv preprint arXiv:2303.12153 (2023).

    extension://idghocbbahafpfhjnfhpbfbmpegphmmp/assets/pdf/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2303.12153.pdf




## [4] Sharma, Pratyusha, et al. “Correcting robot plans with natural language feedback.” arXiv preprint arXiv:2204.05186 (2022). 



https://github.com/hpcaitech/ColossalAI/blob/main/docs/README-zh-Hans.md#%E5%8D%95GPU%E8%AE%AD%E7%BB%83%E6%A0%B7%E4%BE%8B%E5%B1%95%E7%A4%BA
