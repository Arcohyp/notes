先说结论，可行，但存在着一些困难。

什么样的模型需要单机多卡？
 - 单卡无法承载的大语言模型本地训练/推演，模型参数在1B级别及以上，如ChatGLM-6B有6B的参数，lora有13B参数

搭建服务器的前提是对服务器的定义要明确。

	服务器 = 硬件 + 软件（操作系统/用户平台）

硬件方面
- 服务器是一台拥有多个PCI-E接口，支持插入多个显卡的电脑。由于需要多个PCI-E接口，所以主板需要定制，可以由如朴赛等公司提供。显卡和硬盘等需要额外购置。
- NVLink是一个连接器，比PCI-E快，不支持40系，并且只支持两两连接，需要单独购买，注意slot间隔。

有以下几种解决方案：
	24G 显存
	1.服务器 + Geforce Titan RTX(20 series) * 4或者8，使用NVLink 2.0 * 2或4
		除显卡外都要购买
	2.服务器 + Geforce RTX 3090/3090 Ti * 4或者8，使用NVLink 3.0 * 2或4
		以上都要购买

显存并没有1 + 1 = 2，实际上1<, <2。如lora模型微调单卡50+G显存可跑，但4张16G显卡则不一定够用

软件层面，根据抽象的等级，有以下几种解决方案：
	1.用户对显卡不敏感，支持多用户隔绝使用，直接使用算力：当前商业平台都无法完全实现
	2.用户对显卡敏感，支持多用户隔绝使用，需要以显卡为单位使用显卡：主流商业算力平台能够提供的服务，需要自主开发，搭建平台和网络端口服务。
	3.服务器 == 超级主机，仅支持单用户独占使用：思路从“构建一个多用户可用的共享算力平台”变成了“搭建一个可以使用多个显卡算力的超级主机”。

隔绝使用：每个用户都会分配一台虚拟主机，在授予的权限内可使用一定存储空间和单/多张显卡；用户与用户之间数据不流通，资源不共享。

用户层面：
单卡代码和多卡代码是不同的，需要对代码进行改写以分配到各个GPU，可以使用：
	1.torch.nn.DataParallel: 仅数据平行，实际运算还是单卡支持
	2.torch.nn.parallel.DistributedDataParallel: DDP学习成本高，部分情况优于DeepSpeed
3.Deepspeed，ZeRO：需要代码满足更高版本的cuda等，比如deepspeed需要cuda 12.2
